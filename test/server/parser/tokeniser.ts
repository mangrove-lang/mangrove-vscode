
import {readFileSync} from 'fs'
import {TextDocument} from 'vscode-languageserver-textdocument'
import {Tokeniser} from '../../../src/server/parser/tokeniser'
import {TokenType} from '../../../src/server/parser/types'

function tokeniserFor(file: string): Tokeniser
{
	const fileName = `${__dirname}/../../cases/tokenisation/${file}`
	const content = readFileSync(fileName, 'utf-8')

	return new Tokeniser(TextDocument.create(fileName, 'mangrove', 1, content))
}

function readValue(tokeniser: Tokeniser, expectedType: TokenType, expectedValue: string)
{
	const token = tokeniser.next()
	expect(token.valid).toBeTruthy()
	expect(token.type).toEqual(expectedType)
	expect(token.value).toEqual(expectedValue)
}

function readEmptyValue(tokeniser: Tokeniser, expectedType: TokenType)
{
	const token = tokeniser.next()
	expect(token.valid).toBeTruthy()
	expect(token.type).toEqual(expectedType)
	expect(token.value.length).toEqual(0)
}

function readNewline(tokeniser: Tokeniser)
{
	const token = tokeniser.next()
	console.log(`expected in newline: ${TokenType[TokenType.newline]}, token: ${TokenType[token.type]}`)
	expect(token.valid).toBeTruthy()
	expect(token.type).toEqual(TokenType.newline)
	expect(token.value.length).toEqual(0)
}

function readInvalid(tokeniser: Tokeniser)
{
	const token = tokeniser.next()
	expect(token.valid).toBeFalsy()
	expect(token.type).toEqual(TokenType.invalid)
	expect(token.value.length).toEqual(0)
}

function readEOF(tokeniser: Tokeniser)
{
	const token = tokeniser.next()
	expect(token.valid).toBeTruthy()
	expect(token.type).toEqual(TokenType.eof)
	expect(token.value.length).toEqual(0)
}

function readWhitespace(tokeniser: Tokeniser)
{
	const token = tokeniser.next()
	expect(token.valid).toBeTruthy()
	expect(token.type).toEqual(TokenType.whitespace)
	if (token.value === ' ' || token.value === '\t')
		expect(token.value.length).not.toEqual(0)
}

function readAssignment(tokeniser: Tokeniser, identValue: string, assignOpValue: string, literalValue: string)
{
	readValue(tokeniser, TokenType.ident, identValue)
	readWhitespace(tokeniser)
	readValue(tokeniser, TokenType.assignOp, assignOpValue)
	readWhitespace(tokeniser)
	readValue(tokeniser, TokenType.intLit, literalValue)
	readNewline(tokeniser)
}

test('Bad file construction', () =>
{
	const tokeniser = new Tokeniser(TextDocument.create('nonexsistingfile', 'mangrove', 1, ''))
	const token = tokeniser.token
	expect(token.valid).toBeFalsy()
	readEOF(tokeniser)
})

test('Integral literals', () =>
{
	const tokeniser = tokeniserFor('integralLiterals.case')
	// Check that the tokeniser start off in an invalid state
	const token = tokeniser.token
	expect(token.valid).toBeFalsy()

	// It is assumed after each test value that a single Linux-style new line follows
	// Consume the first token from the input and start testing tokenisation
	console.info('Checking tokenisation of \'0\'')
	readValue(tokeniser, TokenType.intLit, '0')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'07\'')
	readValue(tokeniser, TokenType.intLit, '07')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'08\'')
	readValue(tokeniser, TokenType.intLit, '08')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'0b1001\'')
	readValue(tokeniser, TokenType.binLit, '1001')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'0b\'')
	readInvalid(tokeniser)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'0c11\'')
	readValue(tokeniser, TokenType.octLit, '11')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'0c\'')
	readInvalid(tokeniser)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'0x95\'')
	readValue(tokeniser, TokenType.hexLit, '95')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'0x\'')
	readInvalid(tokeniser)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'100\'')
	readValue(tokeniser, TokenType.intLit, '100')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'6\'')
	readValue(tokeniser, TokenType.intLit, '6')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'0a\'')
	readValue(tokeniser, TokenType.intLit, '0')
	readValue(tokeniser, TokenType.ident, 'a')
	readNewline(tokeniser)
	// Finally, consume one last token and make sure it's the EOF token
	readEOF(tokeniser)
})

test('String literals', () =>
{
	const tokeniser = tokeniserFor('stringLiterals.case')
	// Check that the tokeniser start off in an invalid state
	const token = tokeniser.token
	expect(token.valid).toBeFalsy()

	// It is assumed after each test value that a single Linux-style new line follows
	// Consume the first token from the input and start testing tokenisation
	console.info('Checking tokenisation of ""')
	readValue(tokeniser, TokenType.stringLit, '')
	readNewline(tokeniser)
	console.info('Checking tokenisation of "The quick brown fox jumped over the lazy dog"')
	readValue(tokeniser, TokenType.stringLit, 'The quick brown fox jumped over the lazy dog')
	readNewline(tokeniser)
	console.info('Checking tokenisation of "\ufffd"')
	readValue(tokeniser, TokenType.stringLit, '\ufffd')
	readNewline(tokeniser)
	console.info('Checking tokenisation of "â›"')
	readValue(tokeniser, TokenType.stringLit, 'â›')
	readNewline(tokeniser)
	console.info('Checking tokenisation of "ðŸ¥­ðŸ’–"')
	readValue(tokeniser, TokenType.stringLit, 'ðŸ¥­ðŸ’–')
	readNewline(tokeniser)
	console.info('Checking tokenisation of "ðŸŽ‰ unicode literals ðŸŽŠ"')
	readValue(tokeniser, TokenType.stringLit, 'ðŸŽ‰ unicode literals ðŸŽŠ')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\'')
	readInvalid(tokeniser)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'ðŸ‘\'')
	readValue(tokeniser, TokenType.charLit, 'ðŸ‘')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'â›\'')
	readValue(tokeniser, TokenType.charLit, 'â›')
	readNewline(tokeniser)
	console.info('Checking tokenisation of "\'"')
	readValue(tokeniser, TokenType.stringLit, '\'')
	readNewline(tokeniser)
	console.info('Checking tokenisation of "\\""')
	readValue(tokeniser, TokenType.stringLit, '"')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\\\'\'')
	readValue(tokeniser, TokenType.charLit, '\'')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'"\'')
	readValue(tokeniser, TokenType.charLit, '"')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\\\\\'')
	readValue(tokeniser, TokenType.charLit, '\\')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\\r\'')
	readValue(tokeniser, TokenType.charLit, '\r')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\\n\'')
	readValue(tokeniser, TokenType.charLit, '\n')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\\b\'')
	readValue(tokeniser, TokenType.charLit, '\b')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\\v\'')
	readValue(tokeniser, TokenType.charLit, '\v')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\\t\'')
	readValue(tokeniser, TokenType.charLit, '\t')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\\a\'')
	// node doesn't recognise '\a'
	readValue(tokeniser, TokenType.charLit, '\x07')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\\f\'')
	readValue(tokeniser, TokenType.charLit, '\f')
	readNewline(tokeniser)
	console.info('Checking tokenisation of "\\\'"')
	readInvalid(tokeniser)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\\"\'')
	readInvalid(tokeniser)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'\\/\'')
	readInvalid(tokeniser)
	readNewline(tokeniser)
	// Finally, consume one last token and make sure it's the EOF token
	readEOF(tokeniser)
})

test('Assignments', () =>
{
	const tokeniser = tokeniserFor('assignments.case')
	// Check that the tokeniser start off in an invalid state
	const token = tokeniser.token
	expect(token.valid).toBeFalsy()

	// It is assumed after each test value that a single Linux-style new line follows
	// Consume the first token from the input and start testing tokenisation
	console.info('Checking tokenisation of \'a = 1\'')
	readAssignment(tokeniser, 'a', '=', '1')
	console.info('Checking tokenisation of \'b += 2\'')
	readAssignment(tokeniser, 'b', '+=', '2')
	console.info('Checking tokenisation of \'c -= 3\'')
	readAssignment(tokeniser, 'c', '-=', '3')
	console.info('Checking tokenisation of \'d *= 4\'')
	readAssignment(tokeniser, 'd', '*=', '4')
	console.info('Checking tokenisation of \'e /= 5\'')
	readAssignment(tokeniser, 'e', '/=', '5')
	console.info('Checking tokenisation of \'f %= 6\'')
	readAssignment(tokeniser, 'f', '%=', '6')
	console.info('Checking tokenisation of \'g &= 7\'')
	readAssignment(tokeniser, 'g', '&=', '7')
	console.info('Checking tokenisation of \'h |= 8\'')
	readAssignment(tokeniser, 'h', '|=', '8')
	console.info('Checking tokenisation of \'i ^= 9\'')
	readAssignment(tokeniser, 'i', '^=', '9')
	console.info('Checking tokenisation of \'j >>= 10\'')
	readAssignment(tokeniser, 'j', '<<=', '10')
	console.info('Checking tokenisation of \'k <<= 11\'')
	readAssignment(tokeniser, 'k', '>>=', '11')
	// Finally, consume one last token and make sure it's the EOF token
	readEOF(tokeniser)
})

test('Keywords', () =>
{
	const tokeniser = tokeniserFor('keywords.case')
	// Check that the tokeniser start off in an invalid state
	const token = tokeniser.token
	expect(token.valid).toBeFalsy()

	// It is assumed after each test value that a single Linux-style new line follows
	// Consume the first token from the input and start testing tokenisation
	console.info('Checking tokenisation of \'true\'')
	readValue(tokeniser, TokenType.boolLit, 'true')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'false\'')
	readValue(tokeniser, TokenType.boolLit, 'false')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'nullptr\'')
	readValue(tokeniser, TokenType.nullptrLit, 'nullptr')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'and\'')
	readValue(tokeniser, TokenType.logicOp, '&')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'or\'')
	readValue(tokeniser, TokenType.logicOp, '|')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'not\'')
	readValue(tokeniser, TokenType.invert, '!')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'eeprom\'')
	readValue(tokeniser, TokenType.locationSpec, 'eeprom')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'flash\'')
	readValue(tokeniser, TokenType.locationSpec, 'flash')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'rom\'')
	readValue(tokeniser, TokenType.locationSpec, 'rom')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'const\'')
	readValue(tokeniser, TokenType.storageSpec, 'const')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'static\'')
	readValue(tokeniser, TokenType.storageSpec, 'static')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'volatile\'')
	readValue(tokeniser, TokenType.storageSpec, 'volatile')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'new\'')
	readValue(tokeniser, TokenType.newStmt, 'new')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'delete\'')
	readValue(tokeniser, TokenType.deleteStmt, 'delete')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'from\'')
	readValue(tokeniser, TokenType.fromStmt, 'from')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'import\'')
	readValue(tokeniser, TokenType.importStmt, 'import')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'as\'')
	readValue(tokeniser, TokenType.asStmt, 'as')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'return\'')
	readValue(tokeniser, TokenType.returnStmt, 'return')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'if\'')
	readValue(tokeniser, TokenType.ifStmt, 'if')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'elif\'')
	readValue(tokeniser, TokenType.elifStmt, 'elif')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'else\'')
	readValue(tokeniser, TokenType.elseStmt, 'else')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'for\'')
	readValue(tokeniser, TokenType.forStmt, 'for')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'while\'')
	readValue(tokeniser, TokenType.whileStmt, 'while')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'do\'')
	readValue(tokeniser, TokenType.doStmt, 'do')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'none\'')
	readValue(tokeniser, TokenType.noneType, 'none')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'class\'')
	readValue(tokeniser, TokenType.classDef, 'class')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'enum\'')
	readValue(tokeniser, TokenType.enumDef, 'enum')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'function\'')
	readValue(tokeniser, TokenType.functionDef, 'function')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'operator\'')
	readValue(tokeniser, TokenType.operatorDef, 'operator')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'public\'')
	readValue(tokeniser, TokenType.visibility, 'public')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'protected\'')
	readValue(tokeniser, TokenType.visibility, 'protected')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'private\'')
	readValue(tokeniser, TokenType.visibility, 'private')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'unsafe\'')
	readValue(tokeniser, TokenType.unsafe, 'unsafe')
	readNewline(tokeniser)
	// Finally, consume one last token and make sure it's the EOF token
	readEOF(tokeniser)
})

test('Punctuation', () =>
{
	const tokeniser = tokeniserFor('punctuation.case')
	// Check that the tokeniser start off in an invalid state
	const token = tokeniser.token
	expect(token.valid).toBeFalsy()

	// It is assumed after each test value that a single Linux-style new line follows
	// Consume the first token from the input and start testing tokenisation
	console.info('Checking tokenisation of \'.\'')
	readEmptyValue(tokeniser, TokenType.dot)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'..\'')
	readEmptyValue(tokeniser, TokenType.dot)
	readEmptyValue(tokeniser, TokenType.dot)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'...\'')
	readEmptyValue(tokeniser, TokenType.ellipsis)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'...\'')
	readEmptyValue(tokeniser, TokenType.ellipsis)
	readEmptyValue(tokeniser, TokenType.dot)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'# Line comment\'')
	readValue(tokeniser, TokenType.comment, ' Line comment')
	readNewline(tokeniser)
	console.info('Checking tokenisation of\'// Other line comment\'')
	readValue(tokeniser, TokenType.comment, ' Other line comment')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'/**/\'')
	readEmptyValue(tokeniser, TokenType.comment)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'/* Partial * line comment */\'')
	readValue(tokeniser, TokenType.comment, ' Partial * line comment ')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'[]\'')
	readEmptyValue(tokeniser, TokenType.leftSquare)
	readEmptyValue(tokeniser, TokenType.rightSquare)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'{}\'')
	readEmptyValue(tokeniser, TokenType.leftBrace)
	readEmptyValue(tokeniser, TokenType.rightBrace)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'()\'')
	readEmptyValue(tokeniser, TokenType.leftParen)
	readEmptyValue(tokeniser, TokenType.rightParen)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \',\'')
	readEmptyValue(tokeniser, TokenType.comma)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \':\'')
	readEmptyValue(tokeniser, TokenType.colon)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \';\'')
	readEmptyValue(tokeniser, TokenType.semi)
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'<\'')
	readValue(tokeniser, TokenType.relOp, '<')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'<=\'')
	readValue(tokeniser, TokenType.relOp, '<=')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'>\'')
	readValue(tokeniser, TokenType.relOp, '>')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'>=\'')
	readValue(tokeniser, TokenType.relOp, '>=')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'!=\'')
	readValue(tokeniser, TokenType.equOp, '!=')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'==\'')
	readValue(tokeniser, TokenType.equOp, '==')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'+\'')
	readValue(tokeniser, TokenType.addOp, '+')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'-\'')
	readValue(tokeniser, TokenType.addOp, '-')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'*\'')
	readValue(tokeniser, TokenType.mulOp, '*')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'/\'')
	readValue(tokeniser, TokenType.mulOp, '/')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'%\'')
	readValue(tokeniser, TokenType.mulOp, '%')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'~\'')
	readValue(tokeniser, TokenType.invert, '~')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'!\'')
	readValue(tokeniser, TokenType.invert, '!')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'&\'')
	readValue(tokeniser, TokenType.bitOp, '&')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'|\'')
	readValue(tokeniser, TokenType.bitOp, '|')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'^\'')
	readValue(tokeniser, TokenType.bitOp, '^')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'&&\'')
	readValue(tokeniser, TokenType.logicOp, '&')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'||\'')
	readValue(tokeniser, TokenType.logicOp, '|')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'>>\'')
	readValue(tokeniser, TokenType.shiftOp, '>>')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'<<\'')
	readValue(tokeniser, TokenType.shiftOp, '<<')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'++\'')
	readValue(tokeniser, TokenType.incOp, '+')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'--\'')
	readValue(tokeniser, TokenType.incOp, '-')
	readNewline(tokeniser)
	console.info('Checking tokenisation of \'->\'')
	readEmptyValue(tokeniser, TokenType.arrow)
	readNewline(tokeniser)
	// Finally, consume one last token and make sure it's the EOF token
	readEOF(tokeniser)
})
